{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "novel-residence",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scrungus/.local/lib/python3.9/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: /home/scrungus/.local/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZNK3c106IValue23reportToTensorTypeErrorEv\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import OrderedDict, deque, namedtuple\n",
    "from typing import List, Tuple\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.utilities import DistributedType\n",
    "from torch import Tensor, nn\n",
    "from torch.optim import Adam, Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import IterableDataset\n",
    "\n",
    "PATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\n",
    "AVAIL_GPUS = min(1, torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "peripheral-hepatitis",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"Simple MLP network.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_size: int, n_actions: int, hidden_size: int = 128):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            obs_size: observation/state size of the environment\n",
    "            n_actions: number of discrete actions available in the environment\n",
    "            hidden_size: size of hidden layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x.float())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "important-industry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named tuple for storing experience steps gathered in training\n",
    "Experience = namedtuple(\n",
    "    \"Experience\",\n",
    "    field_names=[\"state\", \"action\", \"reward\", \"done\", \"new_state\"],\n",
    ")\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Replay Buffer for storing past experiences allowing the agent to learn from them.\n",
    "\n",
    "    Args:\n",
    "        capacity: size of the buffer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int) -> None:\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self) -> None:\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, experience: Experience) -> None:\n",
    "        \"\"\"Add experience to the buffer.\n",
    "\n",
    "        Args:\n",
    "            experience: tuple (state, action, reward, done, new_state)\n",
    "        \"\"\"\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size: int) -> Tuple:\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, dones, next_states = zip(*(self.buffer[idx] for idx in indices))\n",
    "\n",
    "        return (\n",
    "            np.array(states),\n",
    "            np.array(actions),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(dones, dtype=np.bool),\n",
    "            np.array(next_states),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "anonymous-computer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLDataset(IterableDataset):\n",
    "    \"\"\"Iterable Dataset containing the ExperienceBuffer which will be updated with new experiences during training.\n",
    "\n",
    "    Args:\n",
    "        buffer: replay buffer\n",
    "        sample_size: number of experiences to sample at a time\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, buffer: ReplayBuffer, sample_size: int = 200) -> None:\n",
    "        self.buffer = buffer\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "    def __iter__(self) -> Tuple:\n",
    "        states, actions, rewards, dones, new_states = self.buffer.sample(self.sample_size)\n",
    "        for i in range(len(dones)):\n",
    "            yield states[i], actions[i], rewards[i], dones[i], new_states[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "documentary-soccer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class Agent:\n",
    "    \"\"\"Base Agent class handeling the interaction with the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, env: gym.Env, replay_buffer: ReplayBuffer) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            env: training environment\n",
    "            replay_buffer: replay buffer storing experiences\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.reset()\n",
    "        self.state = self.env.reset()\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Resents the environment and updates the state.\"\"\"\n",
    "        self.state = self.env.reset()\n",
    "\n",
    "    def get_action(self, net: nn.Module, epsilon: float, device: str) -> int:\n",
    "        \"\"\"Using the given network, decide what action to carry out using an epsilon-greedy policy.\n",
    "\n",
    "        Args:\n",
    "            net: DQN network\n",
    "            epsilon: value to determine likelihood of taking a random action\n",
    "            device: current device\n",
    "\n",
    "        Returns:\n",
    "            action\n",
    "        \"\"\"\n",
    "        if np.random.random() < epsilon:\n",
    "            action = random.choice([i for i, e in enumerate(self.env.valid_moves()) if e == 1])\n",
    "        else:\n",
    "            state = torch.tensor([self.state])\n",
    "\n",
    "            if device not in [\"cpu\"]:\n",
    "                state = state.cuda(device)\n",
    "\n",
    "            q_values = net(state)\n",
    "            #masking invalid actions\n",
    "            mask = torch.tensor(go_env.valid_moves(),dtype=torch.bool)\n",
    "            masked = torch.masked_select(net(t_state),mask)\n",
    "            _, action = torch.max(masked, dim=1)\n",
    "            action = int(action.item())\n",
    "\n",
    "        return action\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def play_step(\n",
    "        self,\n",
    "        net: nn.Module,\n",
    "        epsilon: float = 0.0,\n",
    "        device: str = \"cpu\",\n",
    "    ) -> Tuple[float, bool]:\n",
    "        \"\"\"Carries out a single interaction step between the agent and the environment.\n",
    "\n",
    "        Args:\n",
    "            net: DQN network\n",
    "            epsilon: value to determine likelihood of taking a random action\n",
    "            device: current device\n",
    "\n",
    "        Returns:\n",
    "            reward, done\n",
    "        \"\"\"\n",
    "\n",
    "        action = self.get_action(net, epsilon, device)\n",
    "\n",
    "        # do step in the environment\n",
    "        new_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "        exp = Experience(self.state, action, reward, done, new_state)\n",
    "\n",
    "        self.replay_buffer.append(exp)\n",
    "\n",
    "        self.state = new_state\n",
    "        if done:\n",
    "            self.reset()\n",
    "        return reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "necessary-fetish",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNLightning(LightningModule):\n",
    "    \"\"\"Basic DQN Model.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size: int = 16,\n",
    "        lr: float = 1e-2,\n",
    "        env: str = \"CartPole-v0\",\n",
    "        gamma: float = 0.99,\n",
    "        sync_rate: int = 10,\n",
    "        replay_size: int = 1000,\n",
    "        warm_start_size: int = 1000,\n",
    "        eps_last_frame: int = 1000,\n",
    "        eps_start: float = 1.0,\n",
    "        eps_end: float = 0.01,\n",
    "        episode_length: int = 200,\n",
    "        warm_start_steps: int = 4,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch_size: size of the batches\")\n",
    "            lr: learning rate\n",
    "            env: gym environment tag\n",
    "            gamma: discount factor\n",
    "            sync_rate: how many frames do we update the target network\n",
    "            replay_size: capacity of the replay buffer\n",
    "            warm_start_size: how many samples do we use to fill our buffer at the start of training\n",
    "            eps_last_frame: what frame should epsilon stop decaying\n",
    "            eps_start: starting value of epsilon\n",
    "            eps_end: final value of epsilon\n",
    "            episode_length: max length of an episode\n",
    "            warm_start_steps: max episode reward in the environment\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.env = gym.make('gym_go:go-v1', size=3, komi=0)\n",
    "        obs_size = self.env.observation_space.shape[0]\n",
    "        n_actions = self.env.action_space.n\n",
    "\n",
    "        self.net = DQN(obs_size, n_actions)\n",
    "        self.target_net = DQN(obs_size, n_actions)\n",
    "\n",
    "        self.buffer = ReplayBuffer(self.hparams.replay_size)\n",
    "        self.agent = Agent(self.env, self.buffer)\n",
    "        self.total_reward = 0\n",
    "        self.episode_reward = 0\n",
    "        self.populate(self.hparams.warm_start_steps)\n",
    "\n",
    "    def populate(self, steps: int = 1000) -> None:\n",
    "        \"\"\"Carries out several random steps through the environment to initially fill up the replay buffer with\n",
    "        experiences.\n",
    "\n",
    "        Args:\n",
    "            steps: number of random steps to populate the buffer with\n",
    "        \"\"\"\n",
    "        for i in range(steps):\n",
    "            self.agent.play_step(self.net, epsilon=1.0)\n",
    "            #opponent plays random\n",
    "            self.env.step(self.env.uniform_random_action())\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Passes in a state x through the network and gets the q_values of each action as an output.\n",
    "\n",
    "        Args:\n",
    "            x: environment state\n",
    "\n",
    "        Returns:\n",
    "            q values\n",
    "        \"\"\"\n",
    "        output = self.net(x)\n",
    "        return output\n",
    "\n",
    "    def dqn_mse_loss(self, batch: Tuple[Tensor, Tensor]) -> Tensor:\n",
    "        \"\"\"Calculates the mse loss using a mini batch from the replay buffer.\n",
    "\n",
    "        Args:\n",
    "            batch: current mini batch of replay data\n",
    "\n",
    "        Returns:\n",
    "            loss\n",
    "        \"\"\"\n",
    "        states, actions, rewards, dones, next_states = batch\n",
    "\n",
    "        state_action_values = self.net(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_state_values = self.target_net(next_states).max(1)[0]\n",
    "            next_state_values[dones] = 0.0\n",
    "            next_state_values = next_state_values.detach()\n",
    "\n",
    "        expected_state_action_values = next_state_values * self.hparams.gamma + rewards\n",
    "\n",
    "        return nn.MSELoss()(state_action_values, expected_state_action_values)\n",
    "\n",
    "    def training_step(self, batch: Tuple[Tensor, Tensor], nb_batch) -> OrderedDict:\n",
    "        \"\"\"Carries out a single step through the environment to update the replay buffer. Then calculates loss\n",
    "        based on the minibatch recieved.\n",
    "\n",
    "        Args:\n",
    "            batch: current mini batch of replay data\n",
    "            nb_batch: batch number\n",
    "\n",
    "        Returns:\n",
    "            Training loss and log metrics\n",
    "        \"\"\"\n",
    "        device = self.get_device(batch)\n",
    "        epsilon = max(\n",
    "            self.hparams.eps_end,\n",
    "            self.hparams.eps_start - self.global_step + 1 / self.hparams.eps_last_frame,\n",
    "        )\n",
    "\n",
    "        # step through environment with agent\n",
    "        reward, done = self.agent.play_step(self.net, epsilon, device)\n",
    "        \n",
    "        #opponent plays random\n",
    "        self.env.step(self.env.uniform_random_action())\n",
    "        \n",
    "        self.episode_reward += reward\n",
    "\n",
    "        # calculates training loss\n",
    "        loss = self.dqn_mse_loss(batch)\n",
    "\n",
    "        if self.trainer._distrib_type in {DistributedType.DP, DistributedType.DDP2}:\n",
    "            loss = loss.unsqueeze(0)\n",
    "\n",
    "        if done:\n",
    "            self.total_reward = self.episode_reward\n",
    "            self.episode_reward = 0\n",
    "\n",
    "        # Soft update of target network\n",
    "        if self.global_step % self.hparams.sync_rate == 0:\n",
    "            self.target_net.load_state_dict(self.net.state_dict())\n",
    "\n",
    "        log = {\n",
    "            \"total_reward\": torch.tensor(self.total_reward).to(device),\n",
    "            \"reward\": torch.tensor(reward).to(device),\n",
    "            \"train_loss\": loss,\n",
    "        }\n",
    "        status = {\n",
    "            \"steps\": torch.tensor(self.global_step).to(device),\n",
    "            \"total_reward\": torch.tensor(self.total_reward).to(device),\n",
    "        }\n",
    "\n",
    "        return OrderedDict({\"loss\": loss, \"log\": log, \"progress_bar\": status})\n",
    "\n",
    "    def configure_optimizers(self) -> List[Optimizer]:\n",
    "        \"\"\"Initialize Adam optimizer.\"\"\"\n",
    "        optimizer = Adam(self.net.parameters(), lr=self.hparams.lr)\n",
    "        return [optimizer]\n",
    "\n",
    "    def __dataloader(self) -> DataLoader:\n",
    "        \"\"\"Initialize the Replay Buffer dataset used for retrieving experiences.\"\"\"\n",
    "        dataset = RLDataset(self.buffer, self.hparams.episode_length)\n",
    "        dataloader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "        )\n",
    "        return dataloader\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        \"\"\"Get train loader.\"\"\"\n",
    "        return self.__dataloader()\n",
    "\n",
    "    def get_device(self, batch) -> str:\n",
    "        \"\"\"Retrieve device currently being used by minibatch.\"\"\"\n",
    "        return batch[0].device.index if self.on_gpu else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tamil-average",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "model = DQNLightning()\n",
    "\n",
    "trainer = Trainer(\n",
    "    gpus=AVAIL_GPUS,\n",
    "    max_epochs=200,\n",
    "    val_check_interval=100,\n",
    ")\n",
    "\n",
    "trainer.fit(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
